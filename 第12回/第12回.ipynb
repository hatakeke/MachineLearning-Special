{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-z8UZvDqDXk"
   },
   "source": [
    "---\n",
    "# **機械学習特論　第12回課題**\n",
    "## **【word2vec】単語のベクトル化および単語間の演算**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20309,
     "status": "ok",
     "timestamp": 1733045497781,
     "user": {
      "displayName": "秦幸輝",
      "userId": "01834463221439937300"
     },
     "user_tz": -540
    },
    "id": "B5Hm3esVqDXm",
    "outputId": "1216b321-2452-4227-bc75-53391b6c8d5e"
   },
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "# %pip install --upgrade pip\n",
    "# %pip install numpy\n",
    "# %pip install matplotlib\n",
    "# %pip install pandas\n",
    "# %pip install openpyxl\n",
    "# %pip install sympy\n",
    "# %pip install scipy\n",
    "# %pip install re\n",
    "# %pip install jaconv\n",
    "# %pip install scikit-learn\n",
    "# %pip install statsmodels\n",
    "# %pip install seaborn\n",
    "# %pip install pmdarima\n",
    "# %pip install kneed\n",
    "# %pip install scikit-learn-intelex\n",
    "# %pip install lightgbm\n",
    "# %pip install opencv-python\n",
    "# %pip install ipywidgets\n",
    "# %pip install gensim\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import sympy as sp\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "# import jaconv\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "import itertools\n",
    "import matplotlib.dates as mdates\n",
    "# import pmdarima as pm\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import r2_score, f1_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "# from kneed import KneeLocator\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "import time\n",
    "from matplotlib.ticker import LogLocator, LogFormatter\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# from sklearnex import patch_sklearn\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import os\n",
    "# import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "# from tensorflow.keras.layers import Conv1D, Conv2D, Flatten, Dense, MaxPooling2D, MaxPooling1D\n",
    "import cv2\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "# print(tf.__version__)\n",
    "# print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='seaborn')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*dask-expr is not installed.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"dask.dataframe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_g8Q0fVqDXn"
   },
   "source": [
    "### 1. 機械学習による単語のベクトル化\n",
    "　英単語に関しては，GloVe (Global Vectors for Word Representation) というWikipedia 2014と Gigaword 5のテキストデータから学習された英語の単語埋め込みモデルを使用する．単語ベクトルは50次元で表現されており，軽量でありながら高い精度を持つのが特徴である．\n",
    "- ケース感度：非ケースセンシティブ（大文字・小文字を区別しない）\n",
    "- 用途：類似度計算，意味的クラスタリング，情報検索，NLPモデルの初期埋め込みベクトルとして使用可能\n",
    "- 利点：小さい次元数でも，単語の意味的類似性を捕捉可能\n",
    "- 制約：英語に特化しているため，多言語や専門用語には向かない場合あり\n",
    "\n",
    "　日本語に関しては，chiveという主にニュース記事や日本語Webページのデータセットを元に学習された日本語特化の単語埋め込みモデルを使用する．単語ベクトルは90次元で表現されており，日本語特有の語彙や文脈を考慮し，語彙の意味関係を学習するよう設計されている．\n",
    "- ケース感度：日本語特有の漢字，ひらがな，カタカナ，アルファベットなどの表記揺れに対応\n",
    "- 用途：日本語テキストにおける類似度計算，トピックモデリング，日本語NLPタスクのベースモデル\n",
    "- 利点：日本語に特化した設計で，日本語文脈に基づく高い意味的精度を提供\n",
    "- 制約：日本語以外の言語では利用不可＆日本語固有の特殊な語彙や専門用語については十分にカバーされていない可能性あり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.downloader.load('glove-wiki-gigaword-50') # Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\n",
    "wv_jp = gensim.models.KeyedVectors.load('chive-1.2-mc90_gensim/chive-1.2-mc90.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ベクトルを利用した単語間の演算\n",
    "　単語をベクトル化可能としたことにより，単語間で演算ができるようになると考えられる．例えば，英単語「Queen」，「Woman」，「Man」に対して，以下のような方程式を立てる．\n",
    "$$\n",
    "\\text{Queen} - \\text{Woman} = X - \\text{Man}\n",
    "$$\n",
    "これを$X$について解けば，\n",
    "$$\n",
    "X = \\text{Queen} - \\text{Woman} + \\text{Man}\n",
    "$$\n",
    "となり，この結果が「King」などの単語となれば理想的である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n"
     ]
    }
   ],
   "source": [
    "X_word = wv.most_similar(positive=['queen', 'man'], negative=['woman'])[0]\n",
    "print(X_word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "したがって，予想した通り「King」という単語が得られ，機械学習モデルが単語に関する潜在空間に正しく単語を分類できていること，およびそれを利用して正しく演算ができていることが確認できた．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "王女\n"
     ]
    }
   ],
   "source": [
    "X_word = wv_jp.most_similar(positive=['王', '女'], negative=['男'])[0]\n",
    "print(X_word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語版に関しても，同様の結果が得られることが確認できた．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rKEwuqZqDXo"
   },
   "source": [
    "### 2. 単語の様々な演算例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ・種族変換\n",
    "$$\n",
    "\\text{Italy} - \\text{Pizza} = \\text{Japan} - X\n",
    "$$\n",
    "$$\n",
    "X = \\text{Pizza} + \\text{Japan} -\\text{Italy} \n",
    "$$\n",
    "$$\n",
    "X = \\text{Sushi} ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sushi', 0.6840553879737854), ('snack', 0.6649713516235352), ('noodle', 0.6522372364997864), ('toy', 0.6270018219947815), ('oyster', 0.6267036199569702), ('tempura', 0.6250739097595215), ('candy', 0.6205143928527832), ('cracker', 0.6172831654548645), ('sandwich', 0.6168217658996582), ('fried', 0.6153010725975037)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=['pizza', 'japan'], negative=['italy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{雷神} - \\text{雷} =  X + \\text{風}\n",
    "$$\n",
    "$$\n",
    "X = \\text{雷神} - \\text{雷} + \\text{風} \n",
    "$$\n",
    "$$\n",
    "X = \\text{風神} ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('風神', 0.5186702013015747), ('テースト', 0.4441182613372803), ('現代風', 0.4287065267562866), ('和風', 0.42711177468299866), ('アレンジ', 0.42209291458129883), ('今風', 0.41361677646636963), ('異国風', 0.4071023762226105), ('シノワズリー', 0.4044426679611206), ('感じ', 0.40338730812072754), ('オリジナル', 0.396009236574173)]\n"
     ]
    }
   ],
   "source": [
    "print(wv_jp.most_similar(positive=['雷神', '風'], negative=['雷']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上より，2つの単語の組を用いてそれぞれに対応するような単語が連想される場合，word2vecによって上手くそれらを推定することができるとわかる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ・連想\n",
    "$$\n",
    "\\text{Gravity} + \\text{Apple} = X\n",
    "$$\n",
    "$$\n",
    "X = \\text{Newton} ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('uses', 0.776378333568573), ('product', 0.7461434602737427), ('using', 0.7373896837234497), ('makes', 0.7290935516357422), ('comes', 0.7158106565475464), ('can', 0.7150611281394958), ('it', 0.7125034332275391), ('device', 0.7122382521629333), ('software', 0.7056861519813538), ('produce', 0.7047755122184753)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=['gravity', 'apple']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{猫} + \\text{小説} = X\n",
    "$$\n",
    "$$\n",
    "X = \\text{夏目漱石} ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('短編小説', 0.6366516351699829), ('飼い猫', 0.6274240016937256), ('にゃんこ', 0.6231776475906372), ('猫好き', 0.619087815284729), ('子猫', 0.6177344918251038), ('三毛', 0.6160128712654114), ('漫画', 0.6030644178390503), ('犬', 0.6008184552192688), ('短編', 0.5957843661308289), ('小説家', 0.587696373462677)]\n"
     ]
    }
   ],
   "source": [
    "print(wv_jp.most_similar(positive=['猫', '小説']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2つの単語の和から，それに関する人物を連想することを試みたが，上手く連想されなかった．これはword2vecに学習された各単語がそれぞれの持つ意味の類似性によって潜在空間に分類されるが，特定の単語の組み合わせの和を求めたところで，新たな単語のベクトルを生み出せるわけではないためであると考えられる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ・中間（平均）\n",
    "$$\n",
    "\\frac {\\text{Hot} + \\text{Cold}} {2} = X\n",
    "$$\n",
    "$$\n",
    "X = \\text{?}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cold', 0.949198305606842), ('hot', 0.9487224221229553), ('cool', 0.8803013563156128), ('dry', 0.8115673661231995), ('warm', 0.8100968599319458)]\n"
     ]
    }
   ],
   "source": [
    "midpoint = (wv['hot'] + wv['cold']) / 2\n",
    "print(wv.similar_by_vector(midpoint, topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac {\\text{算数} + \\text{国語}} {2} = X\n",
    "$$\n",
    "$$\n",
    "X = \\text{?}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('算数', 0.9318869709968567), ('国語', 0.9249826073646545), ('数学', 0.7737184762954712), ('教科', 0.7582989931106567), ('現代文', 0.7315290570259094)]\n"
     ]
    }
   ],
   "source": [
    "midpoint = (wv_jp['算数'] + wv_jp['国語']) / 2\n",
    "print(wv_jp.similar_by_vector(midpoint, topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "対の意味を持つ単語同士の平均を求めることによって，どのような単語が得られるか調べた．これによって，全く異なる意味の単語が出力されるのではと予想したが，結果はその両方に近い単語が出力された．これは，単語同士（ベクトル）の平均をとることによって，両単語の異なるベクトルの成分は打ち消され，似ている部分のみが残り，結果として両単語に共通する単語が出力されたためであると考えられる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1TelIslqDXr"
   },
   "source": [
    "### 感想  \n",
    "　単語を上手く潜在空間に分類できていることが面白く，これによって様々な自然言語処理が上手くできていることがわかった．画像や芸術に対してもこのように上手く潜在空間に分類し，より高度な（人間の発想に似たあるいはそれを凌駕する）画像処理が実現すれば面白いと感じた．"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
