{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **機械学習特論　第5回課題**\n",
    "## **【k近傍法・サポートベクターマシン】アヤメ・手書き数字・衣類に関するデータセットの分類**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "# %pip install --upgrade pip\n",
    "# %pip install numpy\n",
    "# %pip install matplotlib\n",
    "# %pip install pandas\n",
    "# %pip install openpyxl\n",
    "# %pip install sympy\n",
    "# %pip install scipy\n",
    "# %pip install re\n",
    "# %pip install jaconv\n",
    "# %pip install scikit-learn\n",
    "# %pip install statsmodels\n",
    "# %pip install seaborn\n",
    "# %pip install pmdarima\n",
    "# %pip install kneed\n",
    "# %pip install scikit-learn-intelex\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import sympy as sp\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "import jaconv \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "import itertools\n",
    "import matplotlib.dates as mdates\n",
    "import pmdarima as pm\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import r2_score, f1_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from kneed import KneeLocator\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "import time\n",
    "from matplotlib.ticker import LogLocator, LogFormatter\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearnex import patch_sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='seaborn')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 課題\n",
    "#### (1) データセット\n",
    "##### ・Irisのデータセット  \n",
    "　簡単のため，今回は2種類の分類のみを扱う．\n",
    "##### ・MNIST（手書き数字）のデータセット\n",
    "　0～9の数字が用意されており，これらを10個に分類する．データ数が多く，学習モデルのテストに用いられることも多い．\n",
    "##### ・Fashion-MNIST（衣類）のデータセット\n",
    "　衣類の写真が用意されたデータセットである．こちらも画像系の学習モデルのテストとして用いられることが多い．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearnデータセットに収録されたiris(アヤメ)のデータセットをロードしてデータフレームを作成\n",
    "def load_iris_data():\n",
    "    data = load_iris()\n",
    "    x = pd.DataFrame(data[\"data\"],columns=data[\"feature_names\"])[50:150]\n",
    "    y = pd.DataFrame(data[\"target\"],columns=[\"target\"])[50:150] # species 1 2 を抽出\n",
    "    x_train, x_test, y_train, y_test  = train_test_split(x, y, test_size=0.2, random_state=1, stratify=y)\n",
    "    return x_train, x_test, y_train, y_test \n",
    "\n",
    "# 手書き文字のデータセットをダウンロードして、実験用データを準備 (70000枚のうち7000枚を利用)\n",
    "def load_mnist_data():\n",
    "    data = fetch_openml('mnist_784', version=1)\n",
    "    x = np.array(data['data'].astype(np.float32))\n",
    "    y = np.array(data['target'].astype(np.int32))\n",
    "    x_train, x_test, y_train, y_test  = train_test_split(x, y, test_size=0.2, random_state=1, stratify=y)\n",
    "    return x_train, x_test, y_train, y_test \n",
    "\n",
    "# Fashion-MNISTデータセットをダウンロードして、実験用データを準備 (70000枚のうち7000枚を利用)\n",
    "def load_fashion_mnist_data():\n",
    "    data = fetch_openml('Fashion-MNIST')\n",
    "    x = np.array(data['data'].astype(np.float32))\n",
    "    y = np.array(data['target'].astype(np.int32))\n",
    "    x_train, x_test, y_train, y_test  = train_test_split(x, y, test_size=0.2, random_state=1, stratify=y)\n",
    "    return x_train, x_test, y_train, y_test \n",
    "\n",
    "# 一括処理のためにデータセットの辞書を作成\n",
    "dataset = {'iris': load_iris_data(), 'mnist': load_mnist_data(), 'fashon-mnist': load_fashion_mnist_data()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 分類モデル\n",
    "##### ・k近傍法\n",
    "　説明変数に関する空間上の距離に従って，分類する手法である．ここで，$ k $は近くにある$ k $個のデータに従いクラスを決定することを意味する．$ k $が小さいとノイズに弱くなるが，大きすぎると精度が悪くなることが知られている．\n",
    "##### ・サポートベクターマシン（SVM）\n",
    "　対象データを2クラスに識別する手法である．一方のクラスのサポートベクターから境界への距離(マージン)と，もう一方のクラスのサポートベクターから境界への距離(マージン)が最大になる境界を採用してクラスを識別する．誤りなく識別できる場合はハードマージン法を，できない場合はソフトマージン法を利用する．\n",
    "\n",
    "#### (3) 条件\n",
    "##### 1) k近傍法：$ k=3$\n",
    "##### 2) SVM：kernel=\"linear\", c=1, 標準化\n",
    "##### 3) SVM：kernel=\"rbf\", c=1, 標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一括処理のためにモデルの辞書を作成\n",
    "model = {\n",
    "    # k近傍法のモデル\n",
    "    'kNN(k=3)':\n",
    "    KNeighborsClassifier(n_neighbors=3, # k を指定 (デフォルトは 5)\n",
    "                        weights='uniform',  # 距離を考慮しない(uniform:デフォルト)、する(distance)\n",
    "                        algorithm='auto', # 近傍点計算アルゴリズム (auto:デフォルト,ball_tree,kd_tree,brute)\n",
    "                        leaf_size=30,  # ball_tree,kd_tree指定時のリーフサイズの設定 (デフォルトは 30)\n",
    "                        p=2),  # 距離計算の次元 (2:デフォルト、1)\n",
    "    # svm (kernel=\"linear\", C=1) のモデル\n",
    "    'SVC(kernel=\"linear\", C=1)':\n",
    "    svm.SVC(kernel=\"linear\", C=1, max_iter=100000, verbose=True, random_state=1),\n",
    "    # svm (kernel=\"rbf\", C=1) のモデル\n",
    "    'SVC(kernel=\"rbf\", C=1)':\n",
    "    svm.SVC(kernel=\"rbf\", C=1, max_iter=100000, verbose=True, random_state=1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## dataset:iris  x_train:80 x_test:20 y_train:80 y_test:20\n",
      "# no scaling\n",
      "dataset:iris model:kNN(k=3) accuracy_score: train_data: 0.9375 test_data: 0.95\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 0.9625 test_data: 0.95\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.975 test_data: 0.95\n",
      "# with scaling\n",
      "dataset:iris model:kNN(k=3) accuracy_score: train_data: 0.95 test_data: 0.95\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 0.9625 test_data: 1.0\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.975 test_data: 0.95\n",
      "## dataset:mnist  x_train:56000 x_test:14000 y_train:56000 y_test:14000\n",
      "# no scaling\n",
      "dataset:mnist model:kNN(k=3) accuracy_score: train_data: 0.98586 test_data: 0.97229\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 0.88802 test_data: 0.84421\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.99013 test_data: 0.98043\n",
      "# with scaling\n",
      "dataset:mnist model:kNN(k=3) accuracy_score: train_data: 0.97164 test_data: 0.94629\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 0.98382 test_data: 0.92214\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.98632 test_data: 0.96607\n",
      "## dataset:fashon-mnist  x_train:56000 x_test:14000 y_train:56000 y_test:14000\n",
      "# no scaling\n",
      "dataset:fashon-mnist model:kNN(k=3) accuracy_score: train_data: 0.91798 test_data: 0.8565\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 0.73402 test_data: 0.70507\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.90973 test_data: 0.89179\n",
      "# with scaling\n",
      "dataset:fashon-mnist model:kNN(k=3) accuracy_score: train_data: 0.91891 test_data: 0.85586\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 0.86473 test_data: 0.81029\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.922 test_data: 0.89607\n"
     ]
    }
   ],
   "source": [
    "# 辞書に格納したデータセットそれぞれについて性能を確認\n",
    "for dataset_key in dataset.keys():\n",
    "    x_train, x_test, y_train, y_test = dataset[dataset_key]\n",
    "    print(f'## dataset:{dataset_key} ',\n",
    "        f'x_train:{len(x_train)} x_test:{len(x_test)} y_train:{len(y_train)} y_test:{len(y_test)}')\n",
    "\n",
    "    # データ標準化なしで性能を測定\n",
    "    print('# no scaling')\n",
    "    # 辞書に格納したモデルそれぞれについて性能を測定\n",
    "    for model_key in model.keys():\n",
    "        # 学習用データを利用してモデルを学習\n",
    "        clf = model[model_key]\n",
    "        clf = clf.fit(x_train, np.array(y_train).ravel()) \n",
    "\n",
    "        # 学習したモデルの性能(正答率)を学習用データと検証用データで評価\n",
    "        predict_train = clf.predict(x_train)\n",
    "        train_score = metrics.accuracy_score(y_train, predict_train)\n",
    "        predict_test = clf.predict(x_test)\n",
    "        test_score = metrics.accuracy_score(y_test, predict_test)\n",
    "        print(f'dataset:{dataset_key} model:{model_key}', \n",
    "            f'accuracy_score: train_data:{train_score: 0.5} test_data:{test_score: 0.5}')\n",
    "\n",
    "    # データを標準化\n",
    "    print('# with scaling')\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # 辞書に格納したモデルそれぞれについて性能を測定\n",
    "    for model_key in model.keys():\n",
    "        # 学習用データを利用してモデルを学習\n",
    "        clf = model[model_key]\n",
    "        clf = clf.fit(x_train, np.array(y_train).ravel()) \n",
    "\n",
    "        # 学習したモデルの性能(正答率)を学習用データと検証用データで評価\n",
    "        predict_train = clf.predict(x_train)\n",
    "        train_score = metrics.accuracy_score(y_train, predict_train)\n",
    "        predict_test = clf.predict(x_test)\n",
    "        test_score = metrics.accuracy_score(y_test, predict_test)\n",
    "        print(f'dataset:{dataset_key} model:{model_key}', \n",
    "            f'accuracy_score: train_data:{train_score: 0.5} test_data:{test_score: 0.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| モデル       | パラメータ | Iris | MNIST | Fashion-MNIST |\n",
    "|--------------|------------|------|-------|---------------|\n",
    "| kNN      | $k=3$ | 0.95 | 0.94629 | 0.85586 |\n",
    "| SVM      | kernel=\"linear\", c=1, 標準化 | 1.00 | 0.92214 | 0.81029 |\n",
    "| SVM      | kernel=\"rbf\", c=1, 標準化 | 0.95 | 0.96607  | 0.89607 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "標準化をしないと警告文が出力されることがあった．また，データセットとモデルによって標準化を行った方が精度が良くなるものと，行わない方が精度が良くなるものの両方が確認できた．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 最適パラメータの探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 警告を無視\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "# グリッドサーチを用いた最適パラメータの探索\n",
    "for dataset_key in dataset.keys():\n",
    "    x_train, x_test, y_train, y_test = dataset[dataset_key]\n",
    "    print(f'## dataset:{dataset_key} ')\n",
    "\n",
    "    # データを標準化\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # k近傍法のグリッドサーチ\n",
    "    if dataset_key == 'iris':\n",
    "        param_grid_knn = {\n",
    "            'n_neighbors': range(1, 81),\n",
    "            'p': range(1, 11)\n",
    "        }\n",
    "        knn = KNeighborsClassifier(weights='uniform', algorithm='auto', leaf_size=30)\n",
    "        grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "        grid_search_knn.fit(x_train, np.array(y_train).ravel())\n",
    "\n",
    "        best_knn_params = grid_search_knn.best_params_\n",
    "        best_knn_score = grid_search_knn.best_score_\n",
    "        print(f\"kNN: Best score: {best_knn_score: 0.5}, Best k: {best_knn_params['n_neighbors']}, Best p: {best_knn_params['p']}\")\n",
    "\n",
    "    # SVMのグリッドサーチ\n",
    "    param_grid_svm = {\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    }\n",
    "    svm_model = svm.SVC(max_iter=100000, random_state=1)\n",
    "    grid_search_svm = GridSearchCV(svm_model, param_grid_svm, cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "    grid_search_svm.fit(x_train, np.array(y_train).ravel())\n",
    "\n",
    "    best_svm_params = grid_search_svm.best_params_\n",
    "    best_svm_score = grid_search_svm.best_score_\n",
    "    print(f\"SVM: Best score: {best_svm_score: 0.5}, Best kernel: {best_svm_params['kernel']}, Best C: {best_svm_params['C']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k近傍法のパラメータは，kの値とp（ミンコフスキー距離の指数部分：距離の種類）を変更し，SVMはカーネルの種類（線形，多項式，ガウシアン，シグモイド）とCの値（誤分類に対するペナルティ：ハード／ソフトマージン）の組み合わせをグリッドサーチを用いて探索し，最適なパラメータを探索することを試みた．しかし，探索時間がかかりすぎて終わらなかったため，少し妥協した探索プログラムに改良した．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## dataset: iris\n",
      "kNN: Best score: 0.95, Best k: 1, Best p: 1\n",
      "SVM: Best score: 1.0, Best kernel: linear, Best C: 0.1\n",
      "## dataset: mnist\n",
      "SVM: Best score: 0.97207, Best kernel: rbf, Best C: 100\n",
      "## dataset: fashon-mnist\n",
      "SVM: Best score: 0.90736, Best kernel: rbf, Best C: 10\n"
     ]
    }
   ],
   "source": [
    "# 警告を無視\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "# グリッドサーチをfor文に変換した最適パラメータの探索\n",
    "for dataset_key in dataset.keys():\n",
    "    x_train, x_test, y_train, y_test = dataset[dataset_key]\n",
    "    print(f'## dataset: {dataset_key}')\n",
    "\n",
    "    # データを標準化\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # k近傍法の探索\n",
    "    if dataset_key == 'iris':\n",
    "        best_knn_score = 0\n",
    "        best_k, best_p = None, None\n",
    "        for k in range(1, 81):\n",
    "            for p in range(1, 11):\n",
    "                knn = KNeighborsClassifier(n_neighbors=k, p=p, weights='uniform', algorithm='auto', leaf_size=30)\n",
    "                knn.fit(x_train, np.array(y_train).ravel())\n",
    "                score = knn.score(x_test, np.array(y_test).ravel())\n",
    "                if score > best_knn_score:\n",
    "                    best_knn_score = score\n",
    "                    best_k, best_p = k, p\n",
    "                if best_knn_score == 1.0:\n",
    "                    break\n",
    "            if best_knn_score == 1.0:\n",
    "                break\n",
    "        print(f\"kNN: Best score: {best_knn_score:0.5}, Best k: {best_k}, Best p: {best_p}\")\n",
    "\n",
    "    # SVMの探索\n",
    "    best_svm_score = 0\n",
    "    best_kernel, best_C = None, None\n",
    "    for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "        svm_model = svm.SVC(kernel=kernel, C=1, max_iter=100000, random_state=1)\n",
    "        svm_model.fit(x_train, np.array(y_train).ravel())\n",
    "        score = svm_model.score(x_test, np.array(y_test).ravel())\n",
    "        if score > best_svm_score:\n",
    "            best_svm_score = score\n",
    "            best_kernel = kernel\n",
    "        if best_svm_score == 1.0:\n",
    "            break\n",
    "    best_svm_score = 0\n",
    "    for c in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "        svm_model = svm.SVC(kernel=best_kernel, C=c, max_iter=100000, random_state=1)\n",
    "        svm_model.fit(x_train, np.array(y_train).ravel())\n",
    "        score = svm_model.score(x_test, np.array(y_test).ravel())\n",
    "        if score > best_svm_score:\n",
    "            best_svm_score = score\n",
    "            best_C = c\n",
    "        if best_svm_score == 1.0:\n",
    "            break\n",
    "    print(f\"SVM: Best score: {best_svm_score:0.5}, Best kernel: {best_kernel}, Best C: {best_C}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　単純なデータセットに対しては，SVMの線形カーネルのソフトマージンが有効であることが分かった．逆にソフトマージンでも正しく分類できるほどデータが単調であった（分類しやすかった）と考えられる．\n",
    "　MNISTのような複雑なデータセットに対しては，ガウシアンカーネルとハードマージンの組み合わせが有効であることが分かった．これは非線形なデータが多く含まれていることに起因すると考えられる．特にMNISTの方はかなり精度の良いモデルが構築できた．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感想  \n",
    "　カーネルやパラメータの違いで精度に大きく影響することが確認できた．今後，深層学習を行ったり，目的を達成するための機械学習モデルを構成する際も，意思を持ってパラメータを選択できるようになるためにも，この辺りのパラメータの意味には注意したいと感じた．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
